---
sidebar: sidebar 
permalink: nvme_esxi_8.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: 'È possibile configurare NVMe over Fabrics (NVMe-of) sugli host iniziatori che eseguono ESXi 8.x e ONTAP come destinazione.' 
---
= Configurazione host NVMe-of per ESXi 8.x con ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
È possibile configurare NVMe over Fabrics (NVMe-of) sugli host iniziatori che eseguono ESXi 8.x e ONTAP come destinazione.



== Supportabilità

* A partire dall'allocazione dello spazio ONTAP 9.16,1 è abilitata per impostazione predefinita per tutti i namespace NVMe appena creati.
* A partire da ONTAP 9.9.1 P3, il protocollo NVMe/FC è supportato per ESXi 8 e versioni successive.
* A partire da ONTAP 9.10.1, il protocollo NVMe/TCP è supportato per ONTAP.




== Caratteristiche

* Gli host ESXi Initiator possono eseguire traffico NVMe/FC e FCP attraverso le stesse porte della scheda di rete. Vedere link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] Per un elenco di controller e adattatori FC supportati. Vedere link:https://mysupport.netapp.com/matrix/["Tool di matrice di interoperabilità NetApp"^] per l'elenco più aggiornato delle configurazioni e delle versioni supportate.
* Per ESXi 8.0 e versioni successive, HPP (plugin ad alte prestazioni) è il plug-in predefinito per i dispositivi NVMe.




== Limitazioni note

* Mappatura RDM non supportata.




== Abilitare NVMe/FC

NVMe/FC è attivato per impostazione predefinita nelle release di vSphere.

.Verificare NQN host
Controllare la stringa NQN dell'host ESXi e verificare che corrisponda alla stringa NQN dell'host per il sottosistema corrispondente sull'array ONTAP.

[listing]
----
# esxcli nvme info get
----
Output di esempio:

[listing]
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
[listing]
----
# vserver nvme subsystem host show -vserver nvme_fc
----
Output di esempio:

[listing]
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
Se le stringhe NQN host non corrispondono, utilizzare `vserver nvme subsystem host add` Per aggiornare la stringa NQN host corretta nel sottosistema NVMe ONTAP corrispondente.



== Configurare Broadcom/Emulex e Marvell/Qlogic

Il `lpfc` e a. `qlnativefc` Per impostazione predefinita, i driver di vSphere 8.x dispongono della funzionalità NVMe/FC attivata.

Vedere link:https://mysupport.netapp.com/matrix/["Tool di matrice di interoperabilità"^] per verificare se la configurazione è supportata con il driver o il firmware.



== Validare NVMe/FC

Per validare NVMe/FC, è possibile utilizzare la seguente procedura.

.Fasi
. Verificare che l'adattatore NVMe/FC sia presente nell'host ESXi:
+
[listing]
----
# esxcli nvme adapter list
----
+
Output di esempio:

+
[listing]
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----
. Verificare che gli spazi dei nomi NVMe/FC siano stati creati correttamente:
+
Gli UUID nell'esempio seguente rappresentano i dispositivi dello spazio dei nomi NVMe/FC.

+
[listing, subs="+quotes"]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE]
====
In ONTAP 9.7, la dimensione predefinita del blocco per uno spazio dei nomi NVMe/FC è 4K. Questa dimensione predefinita non è compatibile con ESXi. Pertanto, quando si creano spazi dei nomi per ESXi, è necessario impostare la dimensione del blocco dello spazio dei nomi su *512B*. È possibile eseguire questa operazione utilizzando `vserver nvme namespace create` comando.

Esempio,

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Fare riferimento a. link:https://docs.netapp.com/us-en/ontap/concepts/manual-pages.html["Pagine man dei comandi di ONTAP 9"^] per ulteriori dettagli.

====
. Verificare lo stato dei singoli percorsi ANA dei rispettivi dispositivi dello spazio dei nomi NVMe/FC:
+
[listing, subs="+quotes"]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----




== Configurare NVMe/TCP

In ESXi 8.x, i moduli NVMe/TCP richiesti vengono caricati per impostazione predefinita. Per configurare la rete e l'adattatore NVMe/TCP, consultare la documentazione di VMware vSphere.



== Validare NVMe/TCP

Per convalidare NVMe/TCP, seguire la procedura riportata di seguito.

.Fasi
. Verificare lo stato dell'adattatore NVMe/TCP:
+
[listing]
----
esxcli nvme adapter list
----
+
Output di esempio:

+
[listing]
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----
. Recuperare un elenco di connessioni NVMe/TCP:
+
[listing]
----
esxcli nvme controller list
----
+
Output di esempio:

+
[listing]
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----
. Recuperare un elenco del numero di percorsi per uno spazio dei nomi NVMe:
+
[listing, subs="+quotes"]
----
esxcli storage hpp path list -d *uuid.f4f14337c3ad4a639edf0e21de8b88bf*
----
+
Output di esempio:

+
[listing, subs="+quotes"]
----
tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.165:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T0:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.168:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T3:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.166:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T2:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.167:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T1:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}
----




== Disallocare NVMe

Il comando NVMe disallocare è supportato per ESXi 8.0u2 e versioni successive con ONTAP 9.16.1 e versioni successive.

Il supporto disallocare è sempre abilitato per gli namespace NVMe. Disallocare consente inoltre al sistema operativo guest di eseguire operazioni 'UNMAP' (talvolta chiamate 'TRIM') sugli archivi dati VMFS. Le operazioni di disallocare consentono a un host di identificare blocchi di dati non più necessari perché non contengono più dati validi. Il sistema storage può quindi rimuovere quei blocchi di dati in modo che lo spazio possa essere consumato altrove.

.Fasi
. Sull'host ESXi, verificare l'impostazione per DSM disallocare con il supporto TP4040:
+
`esxcfg-advcfg -g /SCSi/NVmeUseDsmTp4040`

+
Il valore previsto è 0.

. Abilitare l'impostazione per DSM con supporto TP4040:
+
`esxcfg-advcfg -s 1 /Scsi/NvmeUseDsmTp4040`

. Verificare che l'impostazione per DSM disallocare con supporto TP4040 sia abilitata:
+
`esxcfg-advcfg -g /SCSi/NVmeUseDsmTp4040`

+
Il valore previsto è 1.



Per ulteriori informazioni su NVMe disallocare in VMware vSphere, fare riferimento a. https://techdocs.broadcom.com/us/en/vmware-cis/vsphere/vsphere/8-0/vsphere-storage-8-0/storage-provisioning-and-space-reclamation-in-vsphere/storage-space-reclamation-in-vsphere.html["Recupero dello spazio di storage in vSphere"^]



== Problemi noti

La configurazione dell'host NVMe-of per ESXi 8.x con ONTAP presenta i seguenti problemi noti:

[cols="10,30,30"]
|===
| ID bug NetApp | Titolo | Descrizione 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | Nodo ONTAP non operativo quando il protocollo NVMe/FC viene utilizzato con ONTAP versione 9.9.1 | ONTAP 9.9.1 ha introdotto il supporto per il comando "ABORT" di NVMe. Quando ONTAP riceve il comando "abortire" per interrompere un comando NVMe fuse in attesa del comando partner, si verifica un'interruzione del nodo ONTAP. Il problema si verifica solo con gli host che utilizzano i comandi NVMe fused (ad esempio ESX) e il trasporto Fibre Channel (FC). 


| 1543660 | L'errore i/o si verifica quando le macchine virtuali Linux che utilizzano gli adattatori vNVMe incontrano una lunga finestra APD (All Paths Down)  a| 
Le macchine virtuali Linux che eseguono vSphere 8.x e versioni successive e che utilizzano adattatori virtuali NVMe (vNVME) riscontrano un errore i/o perché l'operazione di ripetizione vNVMe è disattivata per impostazione predefinita. Per evitare interruzioni sulle macchine virtuali Linux che eseguono kernel meno recenti durante un All Paths Down (APD) o un carico i/o pesante, VMware ha introdotto un "VSCSIDisableNvmeRetry" sintonizzabile per disattivare l'operazione di ripetizione di vNVMe.

|===
.Informazioni correlate
link:https://docs.netapp.com/us-en/ontap-apps-dbs/vmware/vmware-vsphere-overview.html["VMware vSphere con ONTAP"^] link:https://kb.vmware.com/s/article/2031038["Supporto di VMware vSphere 5.x, 6.x e 7.x con NetApp MetroCluster (2031038)"^] link:https://kb.vmware.com/s/article/83370["Supporto di VMware vSphere 6.x e 7.x con sincronizzazione attiva NetApp SnapMirror"^]
